{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb89937b",
      "metadata": {
        "id": "cb89937b"
      },
      "source": [
        "## <font color = 'green'> 1. Метод главных компонент.  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5515a387-a409-4454-bd66-9b124d1bb04d",
      "metadata": {
        "id": "5515a387-a409-4454-bd66-9b124d1bb04d"
      },
      "source": [
        "Метод главных компонент (PCA) - классический метод понижения размерности данных.\n",
        "\n",
        "Пусть задана матрица информации, содержащая векторы с информацией о данных. $$X=\\left(\n",
        "\\begin{array}{cccc}\n",
        " x_{1,1} & x_{1,2} & ... & x_{1,m} \\\\\n",
        " x_{2,1} & x_{2,2} & ... & x_{2,m} \\\\\n",
        " ... & ... & ... & ... \\\\\n",
        " x_{n,1} & x_{4,n} & ... & x_{n,m} \\\\\n",
        "\\end{array}\n",
        "\\right)$$\n",
        "\n",
        "Как видим рассматриваем $n$ сэмплов и $m$ фич (признаков). Задача **понижения размерности** заключается в преобразовании исходных данных, таким образом, чтобы исходная информации хранилась с использованием меньшего числа фич $k<n$ с сохранением её ценности. Этот метот относится к классу методов **обучения без учителя**, поэтому в нём отсутсвуют целевые метки."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16445805-0022-44f0-b4b2-9f94235ebfe7",
      "metadata": {
        "id": "16445805-0022-44f0-b4b2-9f94235ebfe7"
      },
      "source": [
        "Для решения задачи введём некоторые обозначения. Пусть $Y = {y_{1},y_{2},...,y_{n}}$ выборка из генеральной совокупности. *Выборочное среднее* тогда записывается формулой $E(Y) = \\frac{1}{n}  \\sum\\limits_{j=1}^n y_{i} $. Вторая характеристика выборки это *выборочная дисперсия*. Несмещённая оценка дисперции записывается в виде: $D(Y) = \\frac{1}{n-1}  \\sum\\limits_{j=1}^n (y_{i} - E(y))^{2} $.\n",
        "\n",
        "Заметим, что в случае, если $Y$ - центрирована, то есть $E(Y) = 0$, тогда $D(Y) = \\frac{1}{n-1}  \\sum\\limits_{j=1}^n (y_{i})^{2}$. Если представить $Y$ в виде вектор столбца, то формулу дисперсии можно переписать в матричном виде: $D(\\overline{Y}) = \\frac{1}{n-1} \\overline{Y}^{T}\\overline{Y} $.\n",
        "\n",
        "Вернёмся к нашей задаче. Найдем вектор $\\overline{v}$, такой что при проекции данных на этот вектор, мы получим максимальную дисперсию. Этот вектор и будет первой новой компонентой, а полученные проекции сэмплов - координаты по новой компоненте.\n",
        "\n",
        "Одномерный вектор проекции $X\\overline{v}$ - тоже выборка, для которой мы можем посчитать дисперсию. Учитывая несмещённость данных получаем $D(X\\overline{v}) = \\frac{1}{n-1}(X\\overline{v})^{T}X\\overline{v} =  \\overline{v}^{T} \\frac{1}{n-1}X^{T}X\\overline{v} = \\overline{v}^{T} A \\overline{v}$. Обозначим $A=\\frac{1}{n-1}X^{T}X$.\n",
        "\n",
        "Далее получаем задачу оптимизации с условием. Для того, чтобы компонента была наиболее информативна, будем максимизировать дисперсию данных вдоль неё. Условием будет нормированность вектора $\\overline{v}$.\n",
        "\n",
        "$F(\\overline{v}) = \\overline{v}^{T} A \\overline{v}  -> max$\n",
        "\n",
        "$\\overline{v}^{T}\\overline{v} =  1$\n",
        "\n",
        "Составим функцию Лагранжа $L(\\overline{v}) = \\overline{v}^{T} A \\overline{v} - \\lambda (\\overline{v}^{T}\\overline{v} - 1)$.\n",
        "\n",
        "Проверяем необходимое условие условного экстремума, используя матричные производные:  $L(\\overline{v})^{'} = ( A^{T}+A) \\overline{v} - 2\\lambda \\overline{v} = 0$. Учитывая симметричность матрицы $A$ получим итоговое соотношение для компоненты $\\overline{v} $. $$A\\overline{v}=\\lambda\\overline{v}.$$\n",
        "\n",
        "Полученное соотношение - это определение **собственного вектора** матрицы $A$. В качестве вектора главной компоненты возьмём один из них. А если учесть, что для полученного решения дисперсия $D(X\\overline{v})= \\overline{v}^{T} \\lambda \\overline{v} = \\lambda$ (где $\\lambda$ - собственное значение матрицы $A$), то максимальная дсиперсия будет достигаться вдоль собственного вектора, соответсвующего наибольшему собственному значению $A$.  Остальные компоненты также могут получены как собственные векторы, соответствующие собственным значениям в порядке убывания последних.\n",
        "\n",
        "\n",
        "Заметим, что полученная матрица $A$ - есть [матрица ковариации](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%B2%D0%B0%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0) для $X$.\n",
        "\n",
        "Таким образом кратко алгоритм поиска новых компонент выглядит так:\n",
        "\n",
        "1. Центрируем числовые данные.\n",
        "2. Находим ковариационную матрицу.\n",
        "3. Находим её собственные векторы и упорядочиваем их по убываению их собственных значений.\n",
        "4. Выбираем нужное число компонент.\n",
        "5. Проецируем на них данные."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f3611d-517c-4acc-9a74-f85956fe0ee9",
      "metadata": {
        "id": "23f3611d-517c-4acc-9a74-f85956fe0ee9"
      },
      "source": [
        "### <font color = 'red' size = 5>Задание 1 </font>\n",
        "\n",
        "1. Реализуйте функцию для работы метода главных компонент методом собственных значений.\n",
        "2. Сравните результаты её работы со встроенной функцией на искусственных данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8f63a26",
      "metadata": {
        "id": "b8f63a26"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def pca_eigen(X, n_components):\n",
        "    X_centered = X - np.mean(X, axis=0)\n",
        "\n",
        "    cov_matrix = np.cov(X_centered, rowvar=False)\n",
        "\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[sorted_indices]\n",
        "    eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    principal_components = eigenvectors[:, :n_components]\n",
        "\n",
        "    X_pca = np.dot(X_centered, principal_components)\n",
        "\n",
        "    return X_pca, eigenvalues[:n_components], principal_components\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a033046c",
      "metadata": {
        "id": "a033046c"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5)\n",
        "n_components = 2\n",
        "X_pca_custom, eigenvalues_custom, components_custom = pca_eigen(X, n_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9532c346",
      "metadata": {
        "id": "9532c346"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=n_components)\n",
        "X_pca_sklearn = pca.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e260d4",
      "metadata": {
        "id": "02e260d4",
        "outputId": "4a483f0a-d874-4bd3-e053-3367df417d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Проекция данных (реализация):\n",
            " [[ 0.46103832 -0.0280317 ]\n",
            " [-0.0292814  -0.44507485]\n",
            " [ 0.82739285 -0.13994479]\n",
            " [ 0.15926719 -0.01269403]\n",
            " [-0.24954695  0.22438428]]\n",
            "Проекция данных (sklearn):\n",
            " [[-0.46103832  0.0280317 ]\n",
            " [ 0.0292814   0.44507485]\n",
            " [-0.82739285  0.13994479]\n",
            " [-0.15926719  0.01269403]\n",
            " [ 0.24954695 -0.22438428]]\n",
            "\n",
            "Собственные значения (реализация): [0.12878387 0.10547616]\n",
            "Собственные значения (sklearn): [0.12878387 0.10547616]\n"
          ]
        }
      ],
      "source": [
        "print(\"Проекция данных (реализация):\\n\", X_pca_custom[:5])\n",
        "print(\"Проекция данных (sklearn):\\n\", X_pca_sklearn[:5])\n",
        "\n",
        "print(\"\\nСобственные значения (реализация):\", eigenvalues_custom)\n",
        "print(\"Собственные значения (sklearn):\", pca.explained_variance_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f903aa-66f0-4ece-8001-6de5df4818a6",
      "metadata": {
        "tags": [],
        "id": "67f903aa-66f0-4ece-8001-6de5df4818a6"
      },
      "source": [
        "### <font color = 'red' size = 5>Задание 2 </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fced712c-270c-4dba-b546-9067ecaaf461",
      "metadata": {
        "id": "fced712c-270c-4dba-b546-9067ecaaf461"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c40a6b9-02fb-46a0-b341-63e2ee4c7ea9",
      "metadata": {
        "id": "1c40a6b9-02fb-46a0-b341-63e2ee4c7ea9"
      },
      "source": [
        "1. Используя данные для цветков ириса произвести уменьшение размерности данных с помощью метода главных компонет. Реализовать собственный алгоритм, а также использовать встроенный.\n",
        "2. Оценить степень потери данных.\n",
        "3. Выберите оптимальное количество компонент в новых данных.\n",
        "4. Протестируйте точность различных алгоритмов классификации на исходном датасете и на преобразованном с помощью метода главных компонент. Дайте подробные выводы о различных комбинациях числа компонет и алгоритмах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8c0b5e",
      "metadata": {
        "id": "6f8c0b5e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def pca_eigen(X, n_components):\n",
        "    X_centered = X - np.mean(X, axis=0)\n",
        "\n",
        "    cov_matrix = np.cov(X_centered, rowvar=False)\n",
        "\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[sorted_indices]\n",
        "    eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    principal_components = eigenvectors[:, :n_components]\n",
        "\n",
        "    X_pca = np.dot(X_centered, principal_components)\n",
        "\n",
        "    return X_pca, eigenvalues[:n_components], principal_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe726ac",
      "metadata": {
        "id": "bfe726ac"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "n_components = 2\n",
        "X_train_pca_custom, eigenvalues_custom, _ = pca_eigen(X_train, n_components)\n",
        "X_test_pca_custom, _, _ = pca_eigen(X_test, n_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d4ee32",
      "metadata": {
        "id": "f8d4ee32"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=n_components)\n",
        "X_train_pca_sklearn = pca.fit_transform(X_train)\n",
        "X_test_pca_sklearn = pca.transform(X_test)\n",
        "\n",
        "classifiers = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c31689",
      "metadata": {
        "id": "00c31689"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred_original = clf.predict(X_test)\n",
        "    accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "    clf.fit(X_train_pca_custom, y_train)\n",
        "    y_pred_pca_custom = clf.predict(X_test_pca_custom)\n",
        "    accuracy_pca_custom = accuracy_score(y_test, y_pred_pca_custom)\n",
        "\n",
        "    clf.fit(X_train_pca_sklearn, y_train)\n",
        "    y_pred_pca_sklearn = clf.predict(X_test_pca_sklearn)\n",
        "    accuracy_pca_sklearn = accuracy_score(y_test, y_pred_pca_sklearn)\n",
        "\n",
        "    results[name] = {\n",
        "        \"Original\": accuracy_original,\n",
        "        \"PCA Custom\": accuracy_pca_custom,\n",
        "        \"PCA Sklearn\": accuracy_pca_sklearn\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0aaddda",
      "metadata": {
        "id": "c0aaddda",
        "outputId": "44b1e6bf-1c9f-4a9f-87c6-2be8d5f7ae49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Random Forest:\n",
            "  Original: 1.0000\n",
            "  PCA Custom: 0.9333\n",
            "  PCA Sklearn: 1.0000\n",
            "\n",
            "SVM:\n",
            "  Original: 1.0000\n",
            "  PCA Custom: 0.8667\n",
            "  PCA Sklearn: 0.9778\n",
            "\n",
            "KNN:\n",
            "  Original: 1.0000\n",
            "  PCA Custom: 0.8889\n",
            "  PCA Sklearn: 1.0000\n"
          ]
        }
      ],
      "source": [
        "for clf_name, accuracies in results.items():\n",
        "    print(f\"\\n{clf_name}:\")\n",
        "    for method, accuracy in accuracies.items():\n",
        "        print(f\"  {method}: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b93fdfb",
      "metadata": {
        "id": "6b93fdfb",
        "outputId": "8dafdda4-f5d1-42ad-b5a0-7bc75f56428f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Объясненная дисперсия для каждого компонента (sklearn): [0.9191876  0.05549301]\n",
            "Суммарная объясненная дисперсия для 2 компонент: 0.9746806152027597\n"
          ]
        }
      ],
      "source": [
        "explained_variance_ratios = pca.explained_variance_ratio_\n",
        "print(\"\\nОбъясненная дисперсия для каждого компонента (sklearn):\", explained_variance_ratios)\n",
        "print(\"Суммарная объясненная дисперсия для 2 компонент:\", np.sum(explained_variance_ratios[:2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "335bbc70-c750-4c53-8ead-57246a8666d1",
      "metadata": {
        "id": "335bbc70-c750-4c53-8ead-57246a8666d1"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Выбран кодовый формат\n",
        "```\n",
        "\n",
        "### <font color = 'red' size = 5>Задание 3 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afdc7283-1d70-4d3f-a1f3-aa1e9f815af6",
      "metadata": {
        "id": "afdc7283-1d70-4d3f-a1f3-aa1e9f815af6"
      },
      "source": [
        "Вычислениями подвердите связь [сингулярного разложения матрицы](https://neerc.ifmo.ru/wiki/index.php?title=%D0%A1%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5) с методом главных компонент. Приведите практический пример."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0e1c67",
      "metadata": {
        "id": "fc0e1c67"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def pca_svd(X, n_components):\n",
        "    X_centered = X - np.mean(X, axis=0)\n",
        "\n",
        "    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
        "\n",
        "    principal_components = Vt[:n_components].T\n",
        "\n",
        "    X_pca = np.dot(X_centered, principal_components)\n",
        "\n",
        "    return X_pca, S[:n_components], principal_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a2fbc1",
      "metadata": {
        "id": "50a2fbc1"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.rand(10, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e383ec",
      "metadata": {
        "id": "f3e383ec"
      },
      "outputs": [],
      "source": [
        "n_components = 2\n",
        "X_pca_svd, singular_values, components_svd = pca_svd(X, n_components)\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca_sklearn = pca.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c71afb",
      "metadata": {
        "id": "a3c71afb",
        "outputId": "cb57f367-93b4-4bdc-d5c9-6ec917a5a1ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Проекция данных (через SVD):\n",
            " [[-0.56141715 -0.16712381]\n",
            " [-0.01600634  0.61358387]\n",
            " [-0.88469781  0.08312493]\n",
            " [-0.17512929  0.14441486]\n",
            " [ 0.25376338 -0.04229751]\n",
            " [ 0.09108191 -0.36621103]\n",
            " [ 0.7391968   0.24596106]\n",
            " [ 0.44240949 -0.26632862]\n",
            " [ 0.09652537 -0.01912396]\n",
            " [ 0.01427363 -0.2259998 ]]\n",
            "Проекция данных (sklearn):\n",
            " [[ 0.56141715 -0.16712381]\n",
            " [ 0.01600634  0.61358387]\n",
            " [ 0.88469781  0.08312493]\n",
            " [ 0.17512929  0.14441486]\n",
            " [-0.25376338 -0.04229751]\n",
            " [-0.09108191 -0.36621103]\n",
            " [-0.7391968   0.24596106]\n",
            " [-0.44240949 -0.26632862]\n",
            " [-0.09652537 -0.01912396]\n",
            " [-0.01427363 -0.2259998 ]]\n",
            "\n",
            "Сингулярные значения (через SVD): [1.3975538  0.86657369]\n",
            "Собственные значения (sklearn): [0.2170174  0.08343888]\n",
            "\n",
            "Квадраты сингулярных значений / (n-1): [0.2170174  0.08343888]\n",
            "Собственные значения из sklearn: [0.2170174  0.08343888]\n"
          ]
        }
      ],
      "source": [
        "print(\"Проекция данных (через SVD):\\n\", X_pca_svd)\n",
        "print(\"Проекция данных (sklearn):\\n\", X_pca_sklearn)\n",
        "\n",
        "print(\"\\nСингулярные значения (через SVD):\", singular_values)\n",
        "print(\"Собственные значения (sklearn):\", pca.explained_variance_)\n",
        "\n",
        "print(\"\\nКвадраты сингулярных значений / (n-1):\", (singular_values**2) / (X.shape[0] - 1))\n",
        "print(\"Собственные значения из sklearn:\", pca.explained_variance_)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}